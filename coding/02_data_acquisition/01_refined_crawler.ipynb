{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_and_parse_wikipedia(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "            \n",
    "            content = soup.find('main', {'id': 'content'})\n",
    "            paragraphs = content.find_all('p')\n",
    "            content_text = [paragraph.text for paragraph in paragraphs]\n",
    "            \n",
    "            # Links der Seite extrahieren\n",
    "            links = [a['href'] for a in content.find_all('a', href=True)]\n",
    "            \n",
    "            # Informationen in ein Data Frame speichern\n",
    "            data = pd.DataFrame({'Title': [title], 'Content': ['\\n'.join(content_text)], 'Links': [', '.join(links)]})\n",
    "            \n",
    "            return data\n",
    "        else:\n",
    "            print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_url = \"https://en.wikipedia.org/wiki/Web_crawler\"\n",
    "crawler_data = fetch_and_parse_wikipedia(wikipedia_url)\n",
    "crawler_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_list = [link.strip() for link in crawler_data['Links'].iloc[0].split(', ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe\n",
    "\n",
    "Erstelle ein Skript, welches den obigen Crawler rekursiv aufruft, um auch die jeweils enthaltenen Links zu öffnen und deren Inhalt zu laden. Die Inhalte sollen hierbei stets in ein bestehendes Pandas-Dataframe gespeichert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier Lösung eintragen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('hector')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "442f59ff6058100981594e9535d28b46fc243840058aa6758dcff500d21b0d70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
