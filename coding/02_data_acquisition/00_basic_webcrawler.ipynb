{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installieren der benötigten Abhängigkeiten\n",
    "\n",
    "Um Webseiten programmatisch aufrufen zu können, nutzen wir die Python-Libraries ```requests``` und ```Beautiful Soup```. Erstere dient dazu, überhaupt eine Verbindung zwischen dem lokalen Script und einem Webserver aufbauen zu können. Hierzu stellt die Library ein paar Methoden bereit, um eine HTTP-Verbindung zu einem Server herzustellen und die Daten entsprechend abzurufen. Beautiful Soup hingegen ermöglicht es, den HTML-Inhalt einer Webseite programmatisch auszulesen und die enthaltenen Informationen abzurufen sowie entsprechend zu verarbeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wird eine Webseite definiert, welche im Anschluss ausgelesen werden soll:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL der zu untersuchenden Website\n",
    "url = \"https://en.wikipedia.org/wiki/Web_crawler\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Ziel-URL der auszulesenden Webseite definiert wurde, kann diese im Folgenden abgerufen werden. Mittels ```requests.get(url)``` wird der Inhalt an der Adresse entsprechend abgerufen (das Ergebnis entspricht dem HTML-Inhalt der Webseite). Natürlich kann das auch potenziell fehlerhaft sein, insbesondere wenn die Webseite unter der URL beispielsweise nicht erreichbar ist - deshalb ist es nötig, einen Try-Except-Block zu nutzen, um keine Abbrüche zu erhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = requests.get(url)\n",
    "except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da der Inhalt im ```response```-Objekt relativ schwer zu verarbeiten ist, wird im Folgenden der Text in eine HTML-Baumstruktur umgewandelt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die erhaltene Baumstruktur sieht grob so aus: \n",
    "\n",
    "\n",
    "![](https://www.w3schools.com/whatis/img_htmltree.gif)\n",
    "\n",
    "\n",
    "Um in dieser Struktur nun Inhalte, welche von Relevanz sind, zu finden, werden mittels der ```find()```-Funktion einfache Selektoren genutzt. Hiermit kann beispielsweise der Titel der Seite gefunden werden (```soup.find('h1', {'id': 'firstHeading'})```). Da das Ergebnis dieser Abfrage noch HTML ist, wird mittels ```.text``` der textuelle Inhalt ausgelesen.\n",
    "\n",
    "Genau so wird auch der eigentliche Inhalt ausgelesen, welcher in einem HTML-Tag mit der Bezeichnung ```main``` und der ID ```content``` enthalten ist (```soup.find('main', {'id': 'content'})```).\n",
    "\n",
    "Innerhalb des Seiteninhalts existieren mehrere Abschnitte, welche mittels der ```find_all```-Funktion sowie dem zugehörigen HTML-Element (```p```) gefunden werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "print(f\"Title: {title}\\n\")\n",
    "# Extract and print the main content of the page (the article)\n",
    "content = soup.find('main', {'id': 'content'})\n",
    "paragraphs = content.find_all('p')\n",
    "for paragraph in paragraphs:\n",
    "    print(paragraph.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe\n",
    "\n",
    "Erstelle eine Funktion, welche den Inhalt einer beliebigen Wikipedia-Seite abruft und diesen Inhalt entsprechend zurückgibt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Hinweis 1</summary>\n",
    "  \n",
    "  Zunächst muss der Inhalt der Website abgerufen werden. Folgender Codeschnipsel kann hierbei helfen:\n",
    "  \n",
    "  ```Python\n",
    "  requests.get()\n",
    "  ```\n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Hinweis 2</summary>\n",
    "  \n",
    "  Der erhaltene Inhalt muss mittels Beautiful Soup geparsed werden, um les- und abfragbar zu sein:\n",
    "  ```Python\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  ```\n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Hinweis 3</summary>\n",
    "  \n",
    "  Der Inhalt der Webseite muss gefunden werden. Welches Tag enthält diesen? In den Developer Tools des Browsers kann man die HTML-Struktur der Seite anschauen (F12 in Chrome/Safari).\n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier Lösung eintragen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('hector')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "442f59ff6058100981594e9535d28b46fc243840058aa6758dcff500d21b0d70"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
